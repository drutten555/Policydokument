import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Data Preparation
documents = [...]  # List of text documents

# 2. Vectorization
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(documents)

# 3. Topic Modeling
n_topics = 10  # You can choose based on coherence score
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(X)

# Get topic distribution per document
doc_topic_dist = lda.transform(X)

# 4. Coverage Analysis
topic_coverage = doc_topic_dist.mean(axis=0)

# 5. Visualization
plt.figure(figsize=(10, 6))
sns.heatmap(doc_topic_dist, annot=False, cmap='viridis')
plt.title('Document-Topic Distribution')
plt.xlabel('Topics')
plt.ylabel('Documents')
plt.show()

# Word clouds for each topic (optional)
from wordcloud import WordCloud

for topic_idx, topic in enumerate(lda.components_):
    wordcloud = WordCloud()
    wordcloud.generate_from_frequencies(dict(zip(vectorizer.get_feature_names_out(), topic)))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f'Topic {topic_idx}')
    plt.show()