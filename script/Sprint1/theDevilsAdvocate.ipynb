{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas scikit-learn matplotlib seaborn wordcloud tqdm PyPDF2 gensim smart-open nltk python-dotenv langchain langchain_community openai langchain_openai chromadb langchain_huggingface pypdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in files and load the vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs cleaning:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud \n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI # error? \n",
    "from langchain_openai import ChatOpenAI # sub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS, DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "# Wendy's\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "MODEL_NAME_KBLAB = 'KBLab/sentence-bert-swedish-cased'\n",
    "MODEL_NAME_KB = 'KB/bert-base-swedish-cased'\n",
    "MODEL_NAME_INTFLOAT = 'intfloat/multilingual-e5-large-instruct'\n",
    "MODEL_NAME_KB_LAMMA3dot1 = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "\n",
    "PATH_DB = './db'\n",
    "COLLECTION_NAME = 'policy_collection'\n",
    "# FILE_PATH = './src/documents/Alkohol- och drogpolicy.pdf'\n",
    "# DIR_PATH = './src/documents'\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "directory_path_eng = '/Users/kailashdejesushornig/Documents/GitHub/Policydokument/KaisProject/data/all_files_eng/no duplicates'\n",
    "directory_path_swe = '/Users/kailashdejesushornig/Documents/GitHub/Policydokument/KaisProject/data/all_files_swe/no duplicates'\n",
    "\n",
    "DIR_PATH = directory_path_eng\n",
    "doc_names = os.listdir(DIR_PATH)\n",
    "\n",
    "# change '.PDF' to '.pdf'\n",
    "for doc_name in doc_names:\n",
    "    if doc_name.endswith('.PDF'):\n",
    "        old_file_path = os.path.join(DIR_PATH, doc_name)\n",
    "        new_file_name = doc_name[:-4] + '.pdf'\n",
    "        new_file_path = os.path.join(DIR_PATH, new_file_name)\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "\n",
    "# load pdf document. Use PyPDFDirectoryLoader for loading files in directory.\n",
    "loader = PyPDFDirectoryLoader(DIR_PATH)\n",
    "documents = loader.load()\n",
    "documents[:1]\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dir(texts: List[str], file_name: str, dir_path: str ='outputs'):\n",
    "    \"\"\"\n",
    "    Save text output to a .txt file in an `outputs` directory. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an output directory if it doesn't exist.\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        print(\"Folder %s created!\" % dir_path)\n",
    "\n",
    "    # Run this to write the answer to a txt file in the output folder\n",
    "    file_path = dir_path + '/' +  file_name + '.txt'\n",
    "    open(file_path, 'w').close()\n",
    "    for text in texts:\n",
    "        with open(file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(text + \"\\n\\n\")\n",
    "\n",
    "def split_documents(chunk_size, documents, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "    # This list is taken from LangChain's MarkdownTextSplitter class\n",
    "    MARKDOWN_SEPARATORS = [\n",
    "        \"\\n\\n\\n\\n\",\n",
    "        \"\\n\\n\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    # Remove all whitespaces between newlines e.g. \\n \\n \\n \\n --> \\n\\n\\n\\n\n",
    "    for doc in documents:\n",
    "        doc.page_content = re.sub('(?<=\\\\n) (?=\\\\n)', '', doc.page_content)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size // 10,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in documents:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "def upload_data(documents, embedding_model, chunk_size, collection_name, persist_dir):\n",
    "    \"\"\"\n",
    "    Create a Chroma vectorstore from a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the documents to chunks\n",
    "    docs = split_documents(\n",
    "        chunk_size,  # Choose a chunk size adapted to our model\n",
    "        documents,\n",
    "        tokenizer_name=MODEL_NAME_KBLAB,\n",
    "    )\n",
    "\n",
    "    # Write chunk texts to txt file\n",
    "    # chunks = [chunk.page_content for chunk in docs]\n",
    "    # save_to_dir(chunks, 'chunks')\n",
    "    \n",
    "    # Create Chroma DB with document chunks\n",
    "    print(f\"Added {len(docs)} chunks to ChromaDB\")\n",
    "    return Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma vectorstore and Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding models maximimum sequence length (not strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# print(f\"Model's maximum sequence length: {SentenceTransformer(MODEL_NAME_KBLAB).max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   # Check for CUDA enabled GPU\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_NAME_KBLAB, # Provide the pre-trained model's path\n",
    "    model_kwargs={'device':device}, # Pass the model configuration options\n",
    "    encode_kwargs={'normalize_embeddings': True} # Set `True` for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload docuements to ChromaDB and create a vectorstore\n",
    "#### Run this code if database is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 714 chunks to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "# Load pdf document. Use PyPDFDirectoryLoader for loading files in directory.\n",
    "# loader = PyPDFLoader(FILE_PATH)\n",
    "# loader = PyPDFDirectoryLoader(DIR_PATH)\n",
    "# documents = loader.load()\n",
    "# print('Nr. of documents:', len(documents))\n",
    "# print('A Document object:', documents[:1])\n",
    "\n",
    "# Create vectorstore with the documents\n",
    "# vectorstore = upload_data(documents, embedding_model, 768, COLLECTION_NAME, PATH_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize existing persisting storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal.\n",
    "client = chromadb.PersistentClient(path=PATH_DB)\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_model,\n",
    "    client=client\n",
    ")\n",
    "vectorstore.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval and generation: Create an agent2agent dialogue pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dummy version with meat-eating debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate dialogue saved to debate_dialogue_20240729_163517.txt\n",
      "Time taken: 9.55 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from datetime import datetime\n",
    "import time\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# These agents a real chatterboxes, they need some restrains\n",
    "output_length = 50\n",
    "\n",
    "# Define system prompts for the two agents\n",
    "devil_system_prompt = f\"\"\"\n",
    "You are the Devil's Advocate. You will have a debate with the Angel's Advocate. Your mission is to make your case that eating meat is ethically right. \n",
    "Always meet your opponent's most recent arguement first and indicate this by writing \"Reponse on opponent's arguement: \". \n",
    "Then continue by presenting a new argument to streghthen your own point of view, indicate your own view by writing \"New aguments made: \".\n",
    "You have a total of {output_length} words to give your response. Also, start every new sentence with a new row after a row break. \n",
    "\"\"\"\n",
    "\n",
    "angel_system_prompt = f\"\"\"\n",
    "You are the Angel's Advocate. You will have a debate with the Devil's Advocate. Your mission is to make the case that eating meat is ethically wrong. \n",
    "Always meet your opponent's most recent arguement first and indicate this by writing \"\\n Reponse on opponent's arguement: \". \n",
    "Then continue by presenting a new argument to streghthen your own point of view, indicate your own view by writing \"\\n New aguments made: \".\n",
    "You have a total of {output_length} words to give your response. Also, start every new sentence with a new row after a row break. \n",
    "\"\"\"\n",
    "\n",
    "# Initialize ChatOpenAI instances\n",
    "devil_agent = ChatOpenAI(api_key=openai.api_key, model=\"gpt-4o\")\n",
    "angel_agent = ChatOpenAI(api_key=openai.api_key, model=\"gpt-4o\")\n",
    "\n",
    "# Define initial task prompt for the devil agent\n",
    "task_prompt = \"Discuss how eating meat is ethically right or wrong. You will start by making presenting your point of view on the matter.\"\n",
    "\n",
    "# Function to create a debate between the two agents\n",
    "def run_debate(devil_prompt, angel_prompt, task_prompt, num_rounds=3):\n",
    "    devil_message = task_prompt\n",
    "    dialogue = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        # Devil agent's turn\n",
    "        devil_response = devil_agent.invoke([{\"role\": \"system\", \"content\": devil_prompt}, {\"role\": \"user\", \"content\": devil_message}])\n",
    "        devil_message = devil_response.content\n",
    "        dialogue.append(f\"\\n \\n #####Devil: {devil_message}\")\n",
    "        \n",
    "        # Angel agent's turn\n",
    "        angel_response = angel_agent.invoke([{\"role\": \"system\", \"content\": angel_prompt}, {\"role\": \"user\", \"content\": devil_message}])\n",
    "        angel_message = angel_response.content\n",
    "        dialogue.append(f\"\\n \\n #####Angel: {angel_message}\")\n",
    "        \n",
    "        # Prepare for the next round\n",
    "        devil_message = angel_message\n",
    "\n",
    "    return dialogue\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Run the debate\n",
    "debate_dialogue = run_debate(devil_system_prompt, angel_system_prompt, task_prompt, num_rounds=2)\n",
    "\n",
    "# Verbose: Print the full dialogue\n",
    "# for line in debate_dialogue:\n",
    "    # print(line)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "# Save the dialogue to a .txt file\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f\"debate_dialogue_{current_time}.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    for line in debate_dialogue:\n",
    "        file.write(line + \"\\n\")\n",
    "    file.write(f\"\\nTime taken: {elapsed_time:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Debate dialogue saved to {file_name}\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Could be extended by another LLM analysing an long debate script and summarize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full version for loopholes [Work in progress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "import time\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# These agents a real chatterboxes, they need some restrains\n",
    "output_length = 50 # tokens\n",
    "discussion_theme = 'It is allowed to drink beer in the Chalmers office after 3 pm on Fridays.' # formulate as a thesis\n",
    "devil_mission= 'Your mission is to exploit loopholes in the university´s governing documents.'\n",
    "angel_mission= 'Your mission is to prevent the exploitation of loopholes in the university´s governing documents.'\n",
    "\n",
    "\n",
    "# Define system prompts for the two agents\n",
    "devil_system_prompt = f\"\"\"\n",
    "You are the Devil's Advocate. You will have a debate with the Angel's Advocate. \n",
    "{devil_mission} You'll advocate for the thesis: {discussion_theme}  \n",
    "You must build your arguments on the information given in the documents you have from the vector storage. \n",
    "Find any relevant documents and make a point based on the information in the document(s), don't forget to reference the source by stating a quote from the document.\n",
    "\n",
    "Always meet your opponent's most recent arguement first and indicate this by writing \"Reponse on opponent's arguement: \". \n",
    "Then continue by presenting a new argument to streghthen your own point of view, indicate your own view by writing \"New aguments made: \".\n",
    "You have a total of {output_length} words to give your response. Also, start every new sentence with a new row after a row break. \n",
    "\"\"\"\n",
    "\n",
    "angel_system_prompt = f\"\"\"\n",
    "You are the Angel's Advocate. You will have a debate with the Devil's Advocate. \n",
    "{angel_mission} You'll advocate for the thesis: {discussion_theme} \n",
    "You must build your arguments on the information given in the documents you have from the vector storage. \n",
    "Find the relevant document for the matter, make a point based on the information given in it and reference the source by stating a quote from the documents.\n",
    "\n",
    "Always meet your opponent's most recent arguement first and indicate this by writing \"\\n Reponse on opponent's arguement: \". \n",
    "Then continue by presenting a new argument to streghthen your own point of view, indicate your own view by writing \"\\n New aguments made: \".\n",
    "You have a total of {output_length} words to give your response. Also, start every new sentence with a new row after a row break. \n",
    "\"\"\"\n",
    "\n",
    "# Initialize ChatOpenAI instances\n",
    "devil_agent = ChatOpenAI(api_key=openai.api_key, model=\"gpt-4o\")\n",
    "angel_agent = ChatOpenAI(api_key=openai.api_key, model=\"gpt-4o\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define initial task prompt for the devil agent\n",
    "task_prompt = \"Discuss why the policy does allow you to drink beer during working hours. You will start by making presenting your point of view on the matter.\"\n",
    "\n",
    "# Function to create a debate between the two agents\n",
    "def run_debate(devil_prompt, angel_prompt, task_prompt, num_rounds=3):\n",
    "    devil_message = task_prompt\n",
    "    dialogue = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "\n",
    "        def format_docs(docs): # what does this do? \n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "        print(f\"__________Turn number {round_num}__________\")\n",
    "        \n",
    "        # Devil agent's turn\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} # question here becomes the input for debate\n",
    "            | devil_system_prompt\n",
    "            | devil_agent\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        devil_response = rag_chain.invoke(devil_message)\n",
    "        devil_message = devil_response.content\n",
    "        dialogue.append(f\"\\n \\n #####Devil: {devil_message}\")\n",
    "        \n",
    "        # Angel agent's turn\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | angel_system_prompt\n",
    "            | angel_agent\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        angel_response = angel_agent.invoke(devil_message)\n",
    "        angel_message = angel_response.content\n",
    "        dialogue.append(f\"\\n \\n #####Angel: {angel_message}\")\n",
    "        \n",
    "        # Prepare for the next round\n",
    "        devil_message = angel_message\n",
    "\n",
    "    return dialogue\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the debate\n",
    "debate_dialogue = run_debate(devil_system_prompt, angel_system_prompt, task_prompt, num_rounds=2)\n",
    "\n",
    "# Verbose: Print the full dialogue\n",
    "# for line in debate_dialogue:\n",
    "    # print(line)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Save the dialogue to a .txt file\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f\"debate_dialogue_{current_time}.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    for line in debate_dialogue:\n",
    "        file.write(line + \"\\n\")\n",
    "    file.write(f\"\\nTime taken: {elapsed_time:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Debate dialogue saved to {file_name}\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Could be extended by another LLM analysing an long debate script and summarize it. Or be the judge on who won the debate and what the answer should be.  \n",
    "# For a more sophisticated solution would include a expert panel that could vote on who would be the winner of the debate with different areas/principles to investigate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
