{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/#retrieval-and-generation-generate\n",
    "\n",
    "https://docs.trychroma.com/guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "\n",
    "# Various models\n",
    "MODEL_NAME_KBLAB = 'KBLab/sentence-bert-swedish-cased' # a Swedish-English bilingual model designed for mapping sentences and paragraphs into a dense vector space\n",
    "MODEL_NAME_KB = 'KB/bert-base-swedish-cased' # a Swedish language model based on BERT, developed by the National Library of Sweden (KBLab)\n",
    "MODEL_NAME_INTFLOAT = 'intfloat/multilingual-e5-large-instruct' # a multilingual text embedding model that supports 94 languages\n",
    "\n",
    "PATH_DB = '/Users/kailashdejesushornig/Documents/GitHub/P2_Policydokument/db'\n",
    "COLLECTION_NAME = 'policy'\n",
    "\n",
    "FILE_PATH = './src/documents/Alkohol- och drogpolicy.pdf'\n",
    "DIR_PATH = './src/documents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dir(texts: List[str], file_name: str, dir_path: str ='outputs'):\n",
    "    \"\"\"\n",
    "    Save text output to a .txt file in an `outputs` directory. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an output directory if it doesn't exist.\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        print(\"Folder %s created!\" % dir_path)\n",
    "\n",
    "    # Run this to write the answer to a txt file in the output folder\n",
    "    file_path = dir_path + '/' +  file_name + '.txt'\n",
    "    open(file_path, 'w').close()\n",
    "    for text in texts:\n",
    "        with open(file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(text + \"\\n\\n\")\n",
    "\n",
    "def split_documents(chunk_size, documents, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "    # This list is taken from LangChain's MarkdownTextSplitter class\n",
    "    MARKDOWN_SEPARATORS = [\n",
    "        \"\\n\\n\\n\\n\",\n",
    "        \"\\n\\n\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    # Remove all whitespaces between newlines e.g. \\n \\n \\n \\n --> \\n\\n\\n\\n\n",
    "    for doc in documents:\n",
    "        doc.page_content = re.sub('(?<=\\\\n) (?=\\\\n)', '', doc.page_content)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size // 10,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in documents:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "def upload_data(documents, embedding_model, chunk_size, collection_name, persist_dir):\n",
    "    \"\"\"\n",
    "    Create a Chroma vectorstore from a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the documents to chunks\n",
    "    docs = split_documents(\n",
    "        chunk_size,  # Choose a chunk size adapted to our model\n",
    "        documents,\n",
    "        tokenizer_name=MODEL_NAME_KBLAB,\n",
    "    )\n",
    "\n",
    "    # Write chunk texts to txt file\n",
    "    # chunks = [chunk.page_content for chunk in docs]\n",
    "    # save_to_dir(chunks, 'chunks')\n",
    "    \n",
    "    # Create Chroma DB with document chunks\n",
    "    print(f\"Added {len(docs)} chunks to ChromaDB\")\n",
    "    return Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma vectorstore and Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   # Check for CUDA enabled GPU\n",
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=MODEL_NAME_KBLAB, # Provide the pre-trained model's path\n",
    "#     model_kwargs={'device':device}, # Pass the model configuration options\n",
    "#     encode_kwargs={'normalize_embeddings': True} # Set `True` for cosine similarity\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize existing persisting storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created load_data.py to upload docs to persitent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(id=36e6f6da-91fe-431d-b7f6-1887192dfd75, name=policy_collection), Collection(id=4993b05f-1944-46f2-ba1a-54f053721598, name=policy), Collection(id=5557934c-0ed0-4f34-9872-19a0c2d47521, name=research), Collection(id=fd0b785b-38cd-4665-981b-6937df5bfae1, name=langchain)]\n"
     ]
    }
   ],
   "source": [
    "# # Now we can load the persisted database from disk, and use it as normal.\n",
    "# # Instantiate a persistent chroma client in the persist_directory.\n",
    "# # This will automatically load any previously saved collections.\n",
    "# # Learn more at docs.trychroma.com\n",
    "ollama_ef = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "client_db = chromadb.PersistentClient(path=PATH_DB)\n",
    "\n",
    "#NEW\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "\n",
    "ollama_ef = OllamaEmbeddingFunction(\n",
    "    url=\"http://localhost:11434/api/embeddings\",\n",
    "    model_name=\"mxbai-embed-large\",\n",
    ")\n",
    "\n",
    "print(client_db.list_collections())\n",
    "\n",
    "# # Get the collection.\n",
    "collection = client_db.get_collection(name=COLLECTION_NAME, embedding_function=ollama_ef)\n",
    "# collection.get()[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the persisted database from disk, and use it as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Langchain embeddings\n",
    "ollama_emb = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = Chroma(\n",
    "    client=client_db,\n",
    "    embedding_function=ollama_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompt for every chat instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the system prompt\n",
    "def build_prompt():\n",
    "    template = \"\"\" #Background\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    The context consists of a number of governing documents from a university. They are all in Swedish. \n",
    "    Your task is to act as an expert on the information that they contain. \n",
    "    You will later be asked various questions that should be possible to answer with the contents of the documents. \n",
    "    However, it might be that the question asked cannot be answered based on the documents’ information alone. \n",
    "    You are only allowed to answer questions based on the information from the documents.\n",
    "    \n",
    "    #ADDITION\n",
    "    Answer with as much information as you can find. Keep in mind that some documents may be old and no longer valid. \n",
    "    If a document mentions that it replaces previous documents via its file number, take into account which document is the current valid one and which should prevail. \n",
    "    If you lack information, the information is ambiguous, or the answer for any other reason is uncertain or unclear, state that \"SVARET ÄR INTE SÄKERT” and explain why.\n",
    "    For any answer you give, you are always forced to give supporting quotes and refer to the documents from which they originate.\n",
    "    Answer in Swedish.\n",
    "    Break your answer up into nicely readable paragraphs.\n",
    "\n",
    "    #RESPONSEFOMAT\n",
    "    Start by repeating the question with a sentence.\n",
    "\n",
    "    Provide answers in the format: \n",
    "    - Topic: (e.g. finance, recruitment)\t\n",
    "        - Document title: (include full name of the document including the file extension  + Diarienummer)\n",
    "            - Quote and the page it comes from, as well as an interpretation of the quotation.\n",
    "\n",
    "    For each answer you give, you are always required to provide supporting quotes and refer to the documents from which they are derived.\n",
    "\n",
    "    #DISCLAIMER \n",
    "    End any answer you give with \"Observera att denna information är baserad på min sökning i de dokument som tillhandahålls och att jag kanske inte har hittat alla relevanta policyer eller riktlinjer. Om du är osäker på någon specifik aspekt rekommenderar jag att du kontaktar respektive avdelning på Chalmers eller andra relevanta myndigheter för förtydligande.\"\n",
    "    \n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "    return PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Check if the OPENAI_API_KEY environment variable is set. Prompt the user to set it if not.\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    openai.api_key = input(\n",
    "        \"Please enter your OpenAI API Key. You can get it from https://platform.openai.com/account/api-keys\\n\"\n",
    "    )\n",
    "else:\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize LLM model. Can be switched to other LLM models like llama3.\n",
    "llm = ChatOpenAI(openai_api_key=os.getenv('OPENAI_API_KEY'), model=\"gpt-4o-mini\") #or \"gpt-4o\"\n",
    "#llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Join the document content into one file.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Initialize a RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | build_prompt()\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 1172\n",
      "\tPrompt Tokens: 638\n",
      "\tCompletion Tokens: 534\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "===========================Query====================================\n",
      "Jag är anställd på Chalmers och vill rekrytera en framgångsrik professor från USA till Chalmers. Vilka dokument är relevanta för mig att ha i åtanke när jag bjuder in professorn för att sälja in Chalmers och en ledig tjänst på universitetet? Jag vill:\n",
      "1. flyga över forskaren från USA,\n",
      "2. bjuda in honom och hans fru på middag,\n",
      "3. låta honom bo på det finaste hotellet i Göteborg.\n",
      "Helst vill jag att Chalmers betalar för alltihop, eftersom just den här professorn och vad han kan tillföra skulle vara ovärderligt för Chalmers. Relevanta områden är ekonomi och rekrytering av professorer. \n",
      "\n",
      "===========================Answer===================================\n",
      "Frågan handlar om vilka dokument som är relevanta för rekrytering av en professor och kostnaderna för att bjuda in denne till Chalmers.\n",
      "\n",
      "- Topic: Rekrytering\n",
      "    - Document title: Rekryteringspolicy för Chalmers tekniska högskola, Dnr 123-4567\n",
      "        - \"Rekrytering av professorer ska ske i enlighet med Chalmers rekryteringspolicy som syftar till att attrahera och behålla högkvalitativa forskare.\" (sidan 2). Detta innebär att det finns riktlinjer för hur rekryteringsprocessen ska gå till, vilket inkluderar inbjudningar och förhandlingar med potentiella kandidater.\n",
      "        \n",
      "- Topic: Ekonomi\n",
      "    - Document title: Ekonomihandbok för Chalmers, Dnr 234-5678\n",
      "        - \"Kostnader för inbjudningar och representation bör godkännas av berörd avdelningschef innan de åläggs universitetets budget.\" (sidan 5). Detta tyder på att även om du vill bjuda in professorn och hans fru, samt täcka deras kostnader, måste dessa utgifter godkännas av en avdelningschef.\n",
      "\n",
      "- Topic: Representationspolicy\n",
      "    - Document title: Representationspolicy för Chalmers tekniska högskola, Dnr 345-6789\n",
      "        - \"Representationskostnader ska vara rimliga och stå i proportion till syftet med representationen.\" (sidan 3). Det är viktigt att kostnaderna för middagen och hotellvistelsen kan rättfärdigas som en del av rekryteringsprocessen, vilket innebär att du bör kunna förklara hur detta gynnar Chalmers.\n",
      "\n",
      "Sammanfattningsvis kräver rekryteringsprocessen att du följer Chalmers rekryteringspolicy för att attrahera den önskade professorn. Vidare behöver du beakta ekonomiska och representationspolicyer när det gäller att täcka kostnader för flyg, middag och hotell, då dessa måste godkännas av rätt instans. \n",
      "\n",
      "Observera att denna information är baserad på min sökning i de dokument som tillhandahålls och att jag kanske inte har hittat alla relevanta policyer eller riktlinjer. Om du är osäker på någon specifik aspekt rekommenderar jag att du kontaktar respektive avdelning på Chalmers eller andra relevanta myndigheter för förtydligande.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Jag är anställd på Chalmers och vill rekrytera en framgångsrik professor från USA till Chalmers. Vilka dokument är relevanta för mig att ha i åtanke när jag bjuder in professorn för att sälja in Chalmers och en ledig tjänst på universitetet? Jag vill:\n",
    "1. flyga över forskaren från USA,\n",
    "2. bjuda in honom och hans fru på middag,\n",
    "3. låta honom bo på det finaste hotellet i Göteborg.\n",
    "Helst vill jag att Chalmers betalar för alltihop, eftersom just den här professorn och vad han kan tillföra skulle vara ovärderligt för Chalmers. Relevanta områden är ekonomi och rekrytering av professorer. \n",
    "\"\"\"\n",
    "\n",
    "# get_openai_callback() prints token usage and the cost.\n",
    "if type(llm) == ChatOpenAI:\n",
    "    with get_openai_callback() as cb:\n",
    "        answer = rag_chain.invoke(query)\n",
    "        print(cb)\n",
    "else:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    \n",
    "print(\"===========================Query====================================\")\n",
    "print(query)\n",
    "print(\"===========================Answer===================================\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache from '/Users/kailashdejesushornig/Documents/GitHub/P2_Policydokument/script/gradio_cached_examples/14' directory. If method or examples have changed since last caching, delete this folder to clear cache.\n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://3a8f1b990be7d14d68.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3a8f1b990be7d14d68.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def respond(question,history):\n",
    "    return  rag_chain.invoke(question)\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    respond,\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me question related to the governing documents at Chalmers\", container=False, scale=7),\n",
    "    title=\"Emilia Chatbot\",\n",
    "    examples=[\"Is it allowed to drink beer on campus?\", \n",
    "              \"Can I invite over a professor from the states and let Chalmers pay for his stay?\", \n",
    "              \"My name is Carl XVI Gustaf, can I park my Jaguar anywhere on campus?\"],\n",
    "    cache_examples=True,\n",
    "    retry_btn=None,\n",
    "\n",
    ").launch(share = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save response to an txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_dir([answer], 'outputs', 'answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Chunk Embeddings in 2D (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pacmap\n",
    "# import plotly.express as px\n",
    "# import pandas as pd\n",
    "\n",
    "# def visualize_chunks(query_vector, collection, path_split):\n",
    "#     print('=> Fitting data to 2D...')\n",
    "    \n",
    "#     data = collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "#     df = pd.DataFrame.from_dict(data=data['embeddings'])\n",
    "#     metadatas = data['metadatas']\n",
    "#     documents = data['documents']\n",
    "#     print(metadatas[0]['source'].split(path_split)[1])\n",
    "    \n",
    "#     print('=> Extracting info...')\n",
    "#     embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "\n",
    "#     # Fit the data (the index of transformed data corresponds to the index of the original data)\n",
    "#     documents_projected = embedding_projector.fit_transform(df.to_numpy() + [query_vector], init='pca')\n",
    "#     df = pd.DataFrame.from_dict(\n",
    "#         [\n",
    "#             {\n",
    "#                 'x': documents_projected[i, 0],\n",
    "#                 'y': documents_projected[i, 1],\n",
    "#                 'source': metadatas[i]['source'].split(path_split)[1], # May give error. If so, check the 'source' attribute string and change the split() condition\n",
    "#                 'extract': documents[i][:100] + '...',\n",
    "#                 'symbol': 'circle',\n",
    "#                 'size_col': 0.6,\n",
    "#             }\n",
    "#             for i in range(len(documents))\n",
    "#         ]\n",
    "#         + [\n",
    "#             {\n",
    "#                 'x': documents_projected[-1, 0],\n",
    "#                 'y': documents_projected[-1, 1],\n",
    "#                 'source': 'User query',\n",
    "#                 'extract': query,\n",
    "#                 'size_col': 0.1,\n",
    "#                 'symbol': 'star',\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Visualize the chunk vector embeddings\n",
    "#     print('=> Visualizing...')\n",
    "#     fig = px.scatter(df, x='x', y='y', width=800, height=500,\n",
    "#         color='source',\n",
    "#         hover_data='extract',\n",
    "#         size='size_col',\n",
    "#         symbol='symbol',\n",
    "#         color_discrete_map={'User query': 'black'},\n",
    "#     )\n",
    "#     fig.update_traces(\n",
    "#         marker=dict(opacity=1, line=dict(width=0, color='DarkSlateGrey')),\n",
    "#         selector=dict(mode='markers'),\n",
    "#     )\n",
    "#     fig.update_layout(\n",
    "#         legend_title_text='<b>Chunk source</b>',\n",
    "#         title='<b>2D Projection of Chunk Embeddings via PaCMAP</b>',\n",
    "#     )\n",
    "#     fig.show()\n",
    "\n",
    "# # Embedd a query\n",
    "# query = 'Hur är strålsäkerhetsarbetet organiserat?'\n",
    "# query_vector = embedding_model.embed_query(query)\n",
    "\n",
    "# # Get collection\n",
    "# collection = vectorstore._client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# # Print path to source to get what token to split the path. \n",
    "# # Change `path_split` accordingly in next code cell.\n",
    "# print(collection.get()['metadatas'][0]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize from collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# visualize_chunks(query_vector, collection, path_split='\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
