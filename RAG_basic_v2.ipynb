{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "\n",
    "MODEL_NAME_KBLAB = 'KBLab/sentence-bert-swedish-cased'\n",
    "MODEL_NAME_KB = 'KB/bert-base-swedish-cased'\n",
    "MODEL_NAME_INTFLOAT = 'intfloat/multilingual-e5-large-instruct'\n",
    "\n",
    "PATH_DB = './db'\n",
    "COLLECTION_NAME = 'policy_collection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/P2-subset/Forskarutbildning/Föreskrifter för stipendier för studenter inom Chalmers utbildningsprogram på grund- och avancerad nivå C 2019-0748.PDF'\n",
    "DIR_PATH = 'data/P2-subset/Arbetsmiljö'\n",
    "\n",
    "# load pdf document. Use PyPDFDirectoryLoader for loading files in directory.\n",
    "# loader = PyPDFDirectoryLoader(DIR_PATH)\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "documents = loader.load()\n",
    "documents[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(chunk_size, documents, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "    # This list is taken from LangChain's MarkdownTextSplitter class\n",
    "    MARKDOWN_SEPARATORS = [\n",
    "        \"\\n\\n\\n\\n\",\n",
    "        \"\\n\\n\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    # Remove all whitespaces between newlines e.g. \\n \\n \\n \\n --> \\n\\n\\n\\n\n",
    "    for doc in documents:\n",
    "        doc.page_content = re.sub('(?<=\\\\n) (?=\\\\n)', '', doc.page_content)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size // 10,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in documents:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "def upload_data(docs, embedding_model, chunk_size, collection_name, persist_dir):\n",
    "    \"\"\"\n",
    "    Create a Chroma vectorstore from a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the documents to chunks\n",
    "    docs = split_documents(\n",
    "        chunk_size,  # Choose a chunk size adapted to our model\n",
    "        documents,\n",
    "        tokenizer_name=MODEL_NAME_KBLAB,\n",
    "    )\n",
    "\n",
    "    # Write chunk texts to txt file\n",
    "    open('output/chunks.txt', 'w').close()\n",
    "    for chunk in docs:\n",
    "        with open('output/chunks.txt', 'a', encoding='utf-8') as f:\n",
    "            f.write(chunk.page_content + '\\n\\n')\n",
    "    \n",
    "    # Create Chroma DB with document chunks\n",
    "    print(f\"Added {len(docs)} chunks to ChromaDB\")\n",
    "    return Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    \n",
    "def get_embedding_model(model_name, device):\n",
    "    # Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name, # Provide the pre-trained model's path\n",
    "        model_kwargs={'device':device}, # Pass the model configuration options\n",
    "        encode_kwargs={'normalize_embeddings': True} # Set `True` for cosine similarity\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding models maximimum sequence length (not strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# print(f\"Model's maximum sequence length: {SentenceTransformer(MODEL_NAME_KBLAB).max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   # Check for CUDA enabled GPU\n",
    "embedding_model = get_embedding_model(MODEL_NAME_KBLAB, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload docuements to ChromaDB and create a vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code if database is empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = upload_data(documents, embedding_model, 768, COLLECTION_NAME, PATH_DB)\n",
    "# collection = vectorstore._client.get_or_create_collection(name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize existing persisting storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal.\n",
    "client = chromadb.PersistentClient(path=PATH_DB)\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_model,\n",
    "    client=client\n",
    ")\n",
    "vectorstore.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def build_prompt():\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    The context consists of a number of governing documents from a university. They are all in Swedish. \n",
    "    Your task is to act as an expert on the information that they contain. \n",
    "    You will later be asked various questions that should be possible to answer with the contents of the documents. \n",
    "    However, it might be that the question asked cannot be answered based on the documents’ information alone. \n",
    "    You are only allowed to answer questions based on the information from the documents.\n",
    "    \n",
    "    If you lack information, the information is ambiguous, or the answer for any other reason is uncertain or unclear, state that “the answer is not clear” and explain why.\n",
    "    For any answer you give, you are always forced to give supporting quotes and refer to the documents from which they originate.\n",
    "    Answer in Swedish and break your answer up into nicely readable paragraphs.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "    return PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | build_prompt()\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write response to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Vad är SEB:s roll i förvaltningen av Adlerbertska Stiftelsernas medel?'\n",
    "with get_openai_callback() as cb:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(cb)\n",
    "    print(\"====================================================================\")\n",
    "    print(answer)\n",
    "\n",
    "open('output/answer.txt', 'w').close()\n",
    "with open('output/answer.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Chunk Embeddings in 2D (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pacmap\n",
    "# import plotly.express as px\n",
    "\n",
    "# def visualize_chunks(query_vector, collection):\n",
    "#     print('=> Fitting data to 2D...')\n",
    "    \n",
    "#     data = collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "#     df = pd.DataFrame.from_dict(data=data['embeddings'])\n",
    "#     metadatas = data['metadatas']\n",
    "#     documents = data['documents']\n",
    "#     # print('Size of the dataframe: {}'.format(df.shape))\n",
    "#     # print('Size of the query_vector: {}'.format(len(query_vector)))\n",
    "    \n",
    "#     print('=> Extracting info...')\n",
    "#     embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "\n",
    "#     # Fit the data (the index of transformed data corresponds to the index of the original data)\n",
    "#     documents_projected = embedding_projector.fit_transform(df.to_numpy() + [query_vector], init='pca')\n",
    "#     df = pd.DataFrame.from_dict(\n",
    "#         [\n",
    "#             {\n",
    "#                 'x': documents_projected[i, 0],\n",
    "#                 'y': documents_projected[i, 1],\n",
    "#                 'source': metadatas[i]['source'].split('/')[2], # May give error. If so, check the 'source' attribute string and change the split() condition\n",
    "#                 'extract': documents[i][:100] + '...',\n",
    "#                 'symbol': 'circle',\n",
    "#                 'size_col': 0.6,\n",
    "#             }\n",
    "#             for i in range(len(documents))\n",
    "#         ]\n",
    "#         + [\n",
    "#             {\n",
    "#                 'x': documents_projected[-1, 0],\n",
    "#                 'y': documents_projected[-1, 1],\n",
    "#                 'source': 'User query',\n",
    "#                 'extract': query,\n",
    "#                 'size_col': 0.1,\n",
    "#                 'symbol': 'star',\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Visualize the chunk vector embeddings\n",
    "#     print('=> Visualizing...')\n",
    "#     fig = px.scatter(df, x='x', y='y', width=800, height=500,\n",
    "#         color='source',\n",
    "#         hover_data='extract',\n",
    "#         size='size_col',\n",
    "#         symbol='symbol',\n",
    "#         color_discrete_map={'User query': 'black'},\n",
    "#     )\n",
    "#     fig.update_traces(\n",
    "#         marker=dict(opacity=1, line=dict(width=0, color='DarkSlateGrey')),\n",
    "#         selector=dict(mode='markers'),\n",
    "#     )\n",
    "#     fig.update_layout(\n",
    "#         legend_title_text='<b>Chunk source</b>',\n",
    "#         title='<b>2D Projection of Chunk Embeddings via PaCMAP</b>',\n",
    "#     )\n",
    "#     fig.show()\n",
    "\n",
    "# # Embedd a query\n",
    "# query = 'Hur är strålsäkerhetsarbetet organiserat?'\n",
    "# query_vector = embedding_model.embed_query(query)\n",
    "\n",
    "# # Visualize from collection\n",
    "# visualize_chunks(query_vector, collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
