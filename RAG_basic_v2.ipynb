{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/#retrieval-and-generation-generate\n",
    "\n",
    "https://docs.trychroma.com/guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "MODEL_NAME_KBLAB = 'KBLab/sentence-bert-swedish-cased'\n",
    "MODEL_NAME_KB = 'KB/bert-base-swedish-cased'\n",
    "MODEL_NAME_INTFLOAT = 'intfloat/multilingual-e5-large-instruct'\n",
    "\n",
    "PATH_DB = './db'\n",
    "COLLECTION_NAME = 'policy_collection'\n",
    "\n",
    "FILE_PATH = './src/documents/Alkohol- och drogpolicy.pdf'\n",
    "DIR_PATH = './src/documents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dir(texts: List[str], file_name: str, dir_path: str ='outputs'):\n",
    "    \"\"\"\n",
    "    Save text output to a .txt file in an `outputs` directory. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an output directory if it doesn't exist.\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        print(\"Folder %s created!\" % dir_path)\n",
    "\n",
    "    # Run this to write the answer to a txt file in the output folder\n",
    "    file_path = dir_path + '/' +  file_name + '.txt'\n",
    "    open(file_path, 'w').close()\n",
    "    for text in texts:\n",
    "        with open(file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(text + \"\\n\\n\")\n",
    "\n",
    "def split_documents(chunk_size, documents, tokenizer_name):\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "    # This list is taken from LangChain's MarkdownTextSplitter class\n",
    "    MARKDOWN_SEPARATORS = [\n",
    "        \"\\n\\n\\n\\n\",\n",
    "        \"\\n\\n\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    # Remove all whitespaces between newlines e.g. \\n \\n \\n \\n --> \\n\\n\\n\\n\n",
    "    for doc in documents:\n",
    "        doc.page_content = re.sub('(?<=\\\\n) (?=\\\\n)', '', doc.page_content)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size // 10,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in documents:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "def upload_data(documents, embedding_model, chunk_size, collection_name, persist_dir):\n",
    "    \"\"\"\n",
    "    Create a Chroma vectorstore from a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the documents to chunks\n",
    "    docs = split_documents(\n",
    "        chunk_size,  # Choose a chunk size adapted to our model\n",
    "        documents,\n",
    "        tokenizer_name=MODEL_NAME_KBLAB,\n",
    "    )\n",
    "\n",
    "    # Write chunk texts to txt file\n",
    "    # chunks = [chunk.page_content for chunk in docs]\n",
    "    # save_to_dir(chunks, 'chunks')\n",
    "    \n",
    "    # Create Chroma DB with document chunks\n",
    "    print(f\"Added {len(docs)} chunks to ChromaDB\")\n",
    "    return Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma vectorstore and Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   # Check for CUDA enabled GPU\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_NAME_KBLAB, # Provide the pre-trained model's path\n",
    "    model_kwargs={'device':device}, # Pass the model configuration options\n",
    "    encode_kwargs={'normalize_embeddings': True} # Set `True` for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and upload documents to ChromaDB and create a vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code if database is empty. Comment to not run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf document. Use PyPDFDirectoryLoader for loading files in directory.\n",
    "# loader = PyPDFLoader(FILE_PATH)\n",
    "# loader = PyPDFDirectoryLoader(DIR_PATH)\n",
    "# documents = loader.load()\n",
    "# print('Nr. of documents:', len(documents))\n",
    "# print('A Document object:', documents[:1])\n",
    "\n",
    "# Create vectorstore with the documents\n",
    "# vectorstore = upload_data(documents, embedding_model, 768, COLLECTION_NAME, PATH_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize existing persisting storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the persisted database from disk, and use it as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=PATH_DB)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_model,\n",
    "    client=client\n",
    ")\n",
    "vectorstore.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt():\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    The context consists of a number of governing documents from a university. They are all in Swedish. \n",
    "    Your task is to act as an expert on the information that they contain. \n",
    "    You will later be asked various questions that should be possible to answer with the contents of the documents. \n",
    "    However, it might be that the question asked cannot be answered based on the documents’ information alone. \n",
    "    You are only allowed to answer questions based on the information from the documents.\n",
    "    \n",
    "    If you lack information, the information is ambiguous, or the answer for any other reason is uncertain or unclear, state that “the answer is not clear” and explain why.\n",
    "    For any answer you give, you are always forced to give supporting quotes and refer to the documents from which they originate.\n",
    "    Answer in Swedish.\n",
    "    Break your answer up into nicely readable paragraphs.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "    return PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Check if the OPENAI_API_KEY environment variable is set. Prompt the user to set it if not.\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    openai.api_key = input(\n",
    "        \"Please enter your OpenAI API Key. You can get it from https://platform.openai.com/account/api-keys\\n\"\n",
    "    )\n",
    "else:\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize LLM model. Can be switched to other LLM models like llama3.\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Join the document content into one file.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Initialize a RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | build_prompt()\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Antag att jag arbetar på Chalmers. Jag har en kompis med mig till arbetsplatsen. Jag ska jobba i två timmar till. Får min kompis dricka öl på mitt kontor medans han väntar på mig? '\n",
    "\n",
    "# get_openai_callback() prints token usage and the cost.\n",
    "if type(llm) == ChatOpenAI:\n",
    "    with get_openai_callback() as cb:\n",
    "        answer = rag_chain.invoke(query)\n",
    "        print(cb)\n",
    "else:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    \n",
    "print(\"===========================Query====================================\")\n",
    "print(query)\n",
    "print(\"===========================Answer===================================\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save response to an txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_dir([answer], 'outputs', 'answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Chunk Embeddings in 2D (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_chunks(query_vector, collection, path_split):\n",
    "    print('=> Fitting data to 2D...')\n",
    "    \n",
    "    data = collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "    df = pd.DataFrame.from_dict(data=data['embeddings'])\n",
    "    metadatas = data['metadatas']\n",
    "    documents = data['documents']\n",
    "    print(metadatas[0]['source'].split(path_split)[1])\n",
    "    \n",
    "    print('=> Extracting info...')\n",
    "    embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "\n",
    "    # Fit the data (the index of transformed data corresponds to the index of the original data)\n",
    "    documents_projected = embedding_projector.fit_transform(df.to_numpy() + [query_vector], init='pca')\n",
    "    df = pd.DataFrame.from_dict(\n",
    "        [\n",
    "            {\n",
    "                'x': documents_projected[i, 0],\n",
    "                'y': documents_projected[i, 1],\n",
    "                'source': metadatas[i]['source'].split(path_split)[1], # May give error. If so, check the 'source' attribute string and change the split() condition\n",
    "                'extract': documents[i][:100] + '...',\n",
    "                'symbol': 'circle',\n",
    "                'size_col': 0.6,\n",
    "            }\n",
    "            for i in range(len(documents))\n",
    "        ]\n",
    "        + [\n",
    "            {\n",
    "                'x': documents_projected[-1, 0],\n",
    "                'y': documents_projected[-1, 1],\n",
    "                'source': 'User query',\n",
    "                'extract': query,\n",
    "                'size_col': 0.1,\n",
    "                'symbol': 'star',\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Visualize the chunk vector embeddings\n",
    "    print('=> Visualizing...')\n",
    "    fig = px.scatter(df, x='x', y='y', width=800, height=500,\n",
    "        color='source',\n",
    "        hover_data='extract',\n",
    "        size='size_col',\n",
    "        symbol='symbol',\n",
    "        color_discrete_map={'User query': 'black'},\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        marker=dict(opacity=1, line=dict(width=0, color='DarkSlateGrey')),\n",
    "        selector=dict(mode='markers'),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        legend_title_text='<b>Chunk source</b>',\n",
    "        title='<b>2D Projection of Chunk Embeddings via PaCMAP</b>',\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Embedd a query\n",
    "query = 'Hur är strålsäkerhetsarbetet organiserat?'\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "\n",
    "# Get collection\n",
    "collection = vectorstore._client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# Print path to source to get what token to split the path. \n",
    "# Change `path_split` accordingly in next code cell.\n",
    "print(collection.get()['metadatas'][0]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize from collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "visualize_chunks(query_vector, collection, path_split='\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
