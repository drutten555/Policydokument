{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import getpass\n",
    "import chromadb\n",
    "import openai\n",
    "\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma vectorstore and Embedding model\n",
    "\n",
    "Initialize existing persisting storage.\n",
    "\n",
    "IMPORTANT: make sure to have loaded some documents to the vector database. This can be done by running the ``load_data.py`` in /src."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DB = 'db'\n",
    "COLLECTION_NAME = 'policy'\n",
    "\n",
    "# Instantiate a persistent chroma client in the persist_directory.\n",
    "# This will automatically load any previously saved collections.\n",
    "client_db = chromadb.PersistentClient(path=PATH_DB)\n",
    "\n",
    "# Get the collection.\n",
    "collection = client_db.get_collection(\n",
    "    name=COLLECTION_NAME, \n",
    "    embedding_function=OllamaEmbeddingFunction(\n",
    "        model_name=\"mxbai-embed-large\",\n",
    "        url=\"http://localhost:11434/api/embeddings\",\n",
    "    )\n",
    ")\n",
    "collection.get()[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval and generation: Create an agent2agent dialogue pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(message, docs, is_angel=False):\n",
    "    \"\"\"\n",
    "    Define system prompts for the two agents\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Heres the some documents and a list of the previous debates:\n",
    "        Documents: {docs}\n",
    "        Previous debates: {message}\n",
    "    \"\"\"\n",
    "\n",
    "    restrictions = f\"\"\"You must build your arguments only on the information in the documents.\n",
    "        Answer with as much information as you can find. Keep in mind that some documents may be old and no longer valid.\n",
    "        If a document mentions that it replaces previous documents via its file number, take into account which document is the current valid one and which should prevail.\n",
    "        If you lack information, the information is ambiguous, or the answer for any other reason is uncertain or unclear, state that “the answer is not clear” and explain why.\n",
    "        For any answer you give, you are always forced to give supporting quotes and refer to the source documents.\n",
    "        Answer in Swedish.\n",
    "    \"\"\"\n",
    "\n",
    "    output_format = f\"\"\"\n",
    "        **FIELD NAME (e.g. finance, recruitment)**\n",
    "\n",
    "            ***document title [reference number/diarienummer]***\n",
    "                - Quotation and page number. Include your interpretation of the quotation.\n",
    "\n",
    "        \n",
    "        ***Sammanfattning***\n",
    "        \n",
    "        Summary of the arguments using the quotes. Break it up in nice and readable paragraphs.\\n\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    if not is_angel:\n",
    "        devil_system_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are the Devil's Advocate. You will have a debate with the Angel's Advocate.\n",
    "                    You'll be provided with a list of documents and all the previous debates.\n",
    "                    The documents consists of a number of governing documents from a university.\n",
    "                    Your task is to exploit loopholes in the university´s governing documents for the user's benefit. \n",
    "                    Be specific and detailed on what and how you can exploit the loopholes.\n",
    "                    \n",
    "                    {restrictions}\n",
    "\n",
    "                    #RESPONSEFOMAT\n",
    "                    Provide your response in the following format: \n",
    "                    # DEVIL:\n",
    "                    {output_format}\n",
    "                    \"\"\"\n",
    "            }, \n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        return devil_system_prompt\n",
    "    else:\n",
    "        angel_system_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are the Angel's Advocate. You will have a debate with the Devil's Advocate.\n",
    "                    You'll be provided with a list of documents and all the previous debates.\n",
    "                    The documents consists of a number of governing documents from a university.\n",
    "                    Your task is to argue against the Devil's Advocates arguments to prevent the exploitation of loopholes in the university´s governing documents.\n",
    "\n",
    "                    {restrictions}\n",
    "\n",
    "                    #RESPONSEFOMAT\n",
    "                    Provide your response in the following format:\n",
    "                    # ANGEL:\n",
    "                    {output_format}\n",
    "                \"\"\"\n",
    "            }, \n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    return angel_system_prompt\n",
    "\n",
    "def stream_response(response: ChatCompletion) -> str:\n",
    "    \"\"\"Stream response to output, and convert the response to string.\n",
    "\n",
    "    Args:\n",
    "        response (ChatCompletion): Response from OpenAI chatbot.\n",
    "\n",
    "    Returns:\n",
    "        str: Response message in string.\n",
    "    \"\"\"\n",
    "\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            message += (chunk.choices[0].delta.content)\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "    print(\"\\n\\n\")\n",
    "    return message\n",
    "\n",
    "def get_response(agent: OpenAI, model: str, temperature: float, **kwargs) -> str:\n",
    "    \"\"\"Creates a model response for the given chat conversation and streams the response.\n",
    "\n",
    "    Args:\n",
    "        agent (OpenAI): An openai client instance.\n",
    "        model (str): model name e.g. gpt-4o-mini\n",
    "        temperature (float, optional): What sampling temperature to use, between 0 and 2. \n",
    "            Higher values like 0.8 will make the output more random, while lower values like 0.2 \n",
    "            will make it more focused and deterministic. Defaults to 0.76.\n",
    "\n",
    "    Returns:\n",
    "        str: Response message in string.\n",
    "    \"\"\"\n",
    "\n",
    "    response = agent.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=build_prompt(**kwargs),\n",
    "        stream=True,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    message = stream_response(response)\n",
    "    return message\n",
    "\n",
    "def run_debate(\n",
    "    task_prompt: str, \n",
    "    model: str, \n",
    "    temperature: float = 0.76, \n",
    "    num_rounds: int = 3\n",
    ") -> str:\n",
    "    \n",
    "    # Set OpenAI API key\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY') if 'OPENAI_API_KEY' in os.environ else getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "    # Instantiate the openai clients for each agent. \n",
    "    devil_agent = OpenAI()\n",
    "    angel_agent = OpenAI()\n",
    "    boss_agent = OpenAI()\n",
    "    \n",
    "    # Query the collection to get the 5 most relevant results\n",
    "    task_prompt +=  \"\\nList the reference number/diarienummer for the documents.\"\n",
    "    docs = collection.query(query_texts=[task_prompt], include=[\"documents\", \"metadatas\"])\n",
    "    print(f\"=> Nr. of relevant documents: {len(docs)}\")\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(f\"No relevant docs were retrieved!\")\n",
    "    \n",
    "    # Prepare dialogue.\n",
    "    message = \"# QUERY:\\n\" + task_prompt\n",
    "    dialogue = [message]\n",
    "    print(\"\\n\" + message)\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"__________Turn number {round_num+1}__________\\n\")\n",
    "        \n",
    "        ## Devil agent's turn.\n",
    "        message = get_response(devil_agent, model, temperature, message=dialogue, docs=docs, is_angel=False)\n",
    "        dialogue.append(message)\n",
    "        \n",
    "        ## Angel agent's turn.\n",
    "        message = get_response(angel_agent, model, temperature, message=dialogue, docs=docs, is_angel=True)\n",
    "        dialogue.append(message)\n",
    "\n",
    "        ## Boss agent's turn.\n",
    "        boss_response = boss_agent.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                    Analyze the following debate arguments {dialogue} and make a final decision.\n",
    "                    Start with \"\\n # BOSS:\\n\" followed by your response.\n",
    "                    Answer in Swedish.\n",
    "                \"\"\"\n",
    "            }],\n",
    "            stream=True\n",
    "        )\n",
    "        boss_message = stream_response(boss_response)\n",
    "        dialogue.append(boss_message)\n",
    "\n",
    "    return \"\\n\\n\".join(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define initial task prompt for the devil agent\n",
    "task_alcohol = \"\"\"Discuss why the governing documents does allow you to drink beer during working hours.\"\"\"\n",
    "\n",
    "task_prof = \"\"\"Jag är anställd på Chalmers och vill rekrytera en framgångsrik professor från USA till Chalmers.\n",
    "Vilka dokument är relevanta för mig att ha i åtanke när jag bjuder in professorn för att sälja in Chalmers och en ledig tjänst på universitetet?\n",
    "Jag vill:\n",
    "    1. flyga över forskaren från USA,\n",
    "    2. bjuda in honom och hans fru på middag,\n",
    "    3. låta honom bo på det finaste hotellet i Göteborg.\n",
    "Helst vill jag att Chalmers betalar för alltihop, eftersom just den här professorn och vad han kan tillföra skulle vara ovärderligt för Chalmers. \n",
    "\"\"\"\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "## Run the debate\n",
    "start_time = time.time() # Start the timer\n",
    "\n",
    "debate_dialogue = run_debate(task_prof, MODEL, num_rounds=1)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "## Save the dialogue to a .txt file\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f\"out/debate_dialogue_{current_time}.md\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(debate_dialogue)\n",
    "    file.write(f\"\\n\\n ***Time taken: {elapsed_time:.2f} seconds***\")\n",
    "\n",
    "print(f\"=> Debate dialogue saved to {file_name}\")\n",
    "print(f\"=> Time taken: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
