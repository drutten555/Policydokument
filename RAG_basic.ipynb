{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/#retrieval-and-generation-generate\n",
    "\n",
    "https://docs.trychroma.com/guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Data Preparation\n",
    "PATH_DB = \"db\"                          # path to persistent database\n",
    "PATH_DATA = \"documents/ArbetsmiljoÌˆ\"     # path to documents\n",
    "COLLECTION_NAME = \"policy\"              # name of Chroma collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload documents in directory to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type = \".pdf\"\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(PATH_DATA):\n",
    "    if filename.endswith(file_type.upper()):\n",
    "        # Change '.PDF' to '.pdf'\n",
    "        old_file_path = os.path.join(PATH_DATA, filename)\n",
    "        new_filename = filename.replace(file_type.upper(), file_type)\n",
    "        new_file_path = os.path.join(PATH_DATA, new_filename)\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "    elif filename.endswith(file_type):\n",
    "        # Read all files in the data directory\n",
    "        try:\n",
    "            loader = PyPDFLoader(os.path.join(PATH_DATA, filename))\n",
    "            document = loader.load()\n",
    "            documents.extend(document)\n",
    "        except Exception as e:\n",
    "            print(e, f\"=> Skipping file: {filename}...\", sep=\"\\n\")\n",
    "print(f\"=> Loaded {len(documents)} documents.\")\n",
    "\n",
    "# The langchain embedding model\n",
    "embedder = OllamaEmbeddings(model=\"mxbai-embed-large\")      \n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = SemanticChunker(embedder)\n",
    "new_documents = text_splitter.split_documents(documents)\n",
    "print(f\"=> Split the documents into {len(new_documents)} chunks.\")\n",
    "\n",
    "# Create a Chroma DB with the loaded documents\n",
    "print(f\"=> Loading documents into Chroma DB.\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=new_documents,\n",
    "    embedding=embedder,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PATH_DB\n",
    ")\n",
    "print(f\"Added {len(new_documents)} chunks to the collection: {vectorstore._collection.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize existing persisting storage\n",
    "\n",
    "Note that the embedding function used here is from Chroma and the one used in uploading data to the database is from Langchain.\n",
    "\n",
    "When uploading multiple documents to Chroma, the Langchain's Chroma is used and hence, their respective embedding function is used there. However, when instatiating an existing ChromaDB, then Chroma's own library is used, hence the other embedding function.\n",
    "\n",
    "It is possible to only use one of them, but there were some problems with instantiation a persistent db using Langchain, hence the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a persistent chroma client in the persist_directory.\n",
    "# This will automatically load any previously saved collections.\n",
    "client_db = chromadb.PersistentClient(path=PATH_DB)\n",
    "\n",
    "# Get the collection.\n",
    "collection = client_db.get_collection(\n",
    "    name=COLLECTION_NAME, \n",
    "    embedding_function=OllamaEmbeddingFunction(\n",
    "        model_name=\"mxbai-embed-large\",\n",
    "        url=\"http://localhost:11434/api/embeddings\",\n",
    "    )\n",
    ")\n",
    "collection.get()[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Discuss why the governing documents does allow you to drink beer during working hours.\"\"\"\n",
    "\n",
    "# Query the collection to get the 5 most relevant results\n",
    "results = collection.query(\n",
    "    query_texts=[query], n_results=5, include=[\"documents\", \"metadatas\"]\n",
    ")[\"documents\"][0]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the OPENAI_API_KEY environment variable is set. Prompt the user to set it if not.\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY') if 'OPENAI_API_KEY' in os.environ else getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "openai_client = OpenAI() # defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"\"\"\n",
    "                I am going to ask you a question, which I would like you to answer based only on the provided context, and not any other information.\n",
    "                If there is not enough information in the context to answer the question, say I am not sure, then try to make a guess.\n",
    "                Break your answer up into nicely readable paragraphs.\n",
    "            \"\"\"\n",
    "        }, {\"role\": \"user\", \"content\": f\"Question: {query}. Context: {results}\"}\n",
    "    ],\n",
    ").choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Chunk Embeddings in 2D (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_chunks(query, query_vector, collection, path_split):\n",
    "    print('=> Fitting data to 2D...')    \n",
    "    data = collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "    df = pd.DataFrame.from_dict(data=data['embeddings'])\n",
    "    metadatas = data['metadatas']\n",
    "    documents = data['documents']\n",
    "    \n",
    "    print('=> Extracting info...')\n",
    "    embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "\n",
    "    # Fit the data (the index of transformed data corresponds to the index of the original data)\n",
    "    documents_projected = embedding_projector.fit_transform(df.to_numpy() + [query_vector], init='pca')\n",
    "    df = pd.DataFrame.from_dict(\n",
    "        [\n",
    "            {\n",
    "                'x': documents_projected[i, 0],\n",
    "                'y': documents_projected[i, 1],\n",
    "                'source': metadatas[i]['source'].split(path_split)[1], # May give error. If so, check the 'source' attribute string and change the split() condition\n",
    "                'extract': documents[i][:100] + '...',\n",
    "                'symbol': 'circle',\n",
    "                'size_col': 1,\n",
    "            }\n",
    "            for i in range(len(documents))\n",
    "        ]\n",
    "        + [\n",
    "            {\n",
    "                'x': documents_projected[-1, 0],\n",
    "                'y': documents_projected[-1, 1],\n",
    "                'source': 'User query',\n",
    "                'extract': query,\n",
    "                'size_col': 1,\n",
    "                'symbol': 'star',\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Visualize the chunk vector embeddings\n",
    "    print('=> Visualizing...')\n",
    "    fig = px.scatter(df, x='x', y='y', width=800, height=500,\n",
    "        color='source',\n",
    "        hover_data='extract',\n",
    "        size='size_col',\n",
    "        symbol='symbol',\n",
    "        color_discrete_map={'User query': 'black'},\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        marker=dict(opacity=1, line=dict(width=0, color='DarkSlateGrey')),\n",
    "        selector=dict(mode='markers'),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        legend_title_text='<b>Chunk source</b>',\n",
    "        title='<b>2D Projection of Chunk Embeddings via PaCMAP</b>',\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print path to source to get what token to split the path. \n",
    "Change `path_split` accordingly in next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection\n",
    "collection = client_db.get_collection(COLLECTION_NAME)\n",
    "print(collection.get()['metadatas'][0]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize from collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "query = \"\"\"Discuss why the governing documents does allow you to drink beer during working hours.\"\"\"\n",
    "\n",
    "# Embedd a query\n",
    "query_vector = OllamaEmbeddings(model=\"mxbai-embed-large\").embed_query(query)\n",
    "\n",
    "# Visualize\n",
    "visualize_chunks(query, query_vector, collection, path_split='/')  # Change `path_split` accordingly from previous code cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
